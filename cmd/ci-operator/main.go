package main

import (
	"context"
	"crypto/sha256"
	"encoding/base32"
	"encoding/json"
	"encoding/xml"
	"flag"
	"fmt"
	"io/ioutil"
	"log"
	"os"
	"os/exec"
	"path/filepath"
	"regexp"
	"sort"
	"strings"
	"time"

	"github.com/golang/glog"

	"github.com/openshift/ci-operator/pkg/defaults"
	"github.com/openshift/ci-operator/pkg/load"

	coreapi "k8s.io/api/core/v1"
	rbacapi "k8s.io/api/rbac/v1"
	kerrors "k8s.io/apimachinery/pkg/api/errors"
	meta "k8s.io/apimachinery/pkg/apis/meta/v1"
	coreclientset "k8s.io/client-go/kubernetes/typed/core/v1"
	rbacclientset "k8s.io/client-go/kubernetes/typed/rbac/v1"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/tools/record"
	"k8s.io/client-go/util/retry"

	"github.com/ghodss/yaml"

	imageapi "github.com/openshift/api/image/v1"
	projectapi "github.com/openshift/api/project/v1"
	templateapi "github.com/openshift/api/template/v1"
	buildclientset "github.com/openshift/client-go/build/clientset/versioned/typed/build/v1"
	imageclientset "github.com/openshift/client-go/image/clientset/versioned/typed/image/v1"
	projectclientset "github.com/openshift/client-go/project/clientset/versioned"
	templatescheme "github.com/openshift/client-go/template/clientset/versioned/scheme"
	templateclientset "github.com/openshift/client-go/template/clientset/versioned/typed/template/v1"

	"github.com/openshift/ci-operator/pkg/api"
	"github.com/openshift/ci-operator/pkg/interrupt"
	"github.com/openshift/ci-operator/pkg/junit"
	"github.com/openshift/ci-operator/pkg/steps"
)

const usage = `Orchestrate multi-stage image-based builds

The ci-operator reads a declarative configuration YAML file and executes a set of build
steps on an OpenShift cluster for image-based components. By default, all steps are run,
but a caller may select one or more targets (image names or test names) to limit to only
steps that those targets depend on. The build creates a new project to run the builds in
and can automatically clean up the project when the build completes.

ci-operator leverages declarative OpenShift builds and images to reuse previously compiled
artifacts. It makes building multiple images that share one or more common base layers
simple as well as running tests that depend on those images.

Since the command is intended for use in CI environments it requires an input environment
variable called the JOB_SPEC that defines the GitHub project to execute and the commit,
branch, and any PRs to merge onto the branch. See the kubernetes/test-infra project for
a description of JOB_SPEC.

The inputs of the build (source code, tagged images, configuration) are combined to form
a consistent name for the target namespace that will change if any of the inputs change.
This allows multiple test jobs to share common artifacts and still perform retries.

The standard build steps are designed for simple command-line actions (like invoking
"make test") but can be extended by passing one or more templates via the --template flag.
The name of the template defines the stage and the template must contain at least one
pod. The parameters passed to the template are the current process environment and a set
of dynamic parameters that are inferred from previous steps. These parameters are:

  NAMESPACE
    The namespace generated by the operator for the given inputs or the value of
    --namespace.

  IMAGE_FORMAT
    A string that points to the public image repository URL of the image stream(s)
    created by the tag step. Example:

      registry.svc.ci.openshift.org/ci-op-9o8bacu/stable:${component}

    Will cause the template to depend on all image builds.

  IMAGE_<component>
    The public image repository URL for an output image. If specified the template
    will depend on the image being built.

  LOCAL_IMAGE_<component>
    The public image repository URL for an image that was built during this run but
    was not part of the output (such as pipeline cache images). If specified the
    template will depend on the image being built.

  JOB_NAME
    The job name from the JOB_SPEC

  JOB_NAME_SAFE
    The job name in a form safe for use as a Kubernetes resource name.

  JOB_NAME_HASH
    A short hash of the job name for making tasks unique.

  RPM_REPO_<org>_<repo>
    If the job creates RPMs this will be the public URL that can be used as the
		baseurl= value of an RPM repository. The value of org and repo are uppercased
		and dashes are replaced with underscores.

Dynamic environment variables are overriden by process environment variables.

Both test and template jobs can gather artifacts created by pods. Set
--artifact-dir to define the top level artifact directory, and any test task
that defines artifact_dir or template that has an "artifacts" volume mounted
into a container will have artifacts extracted after the container has completed.
Errors in artifact extraction will not cause build failures.

In CI environments the inputs to a job may be different than what a normal
development workflow would use. The --override file will override fields
defined in the config file, such as base images and the release tag configuration.

After a successful build the --promote will tag each built image (in "images")
to the image stream(s) identified by the "promotion" config. You may add
additional images to promote and their target names via the "additional_images"
map.
`

func main() {
	flagSet := flag.NewFlagSet("", flag.ExitOnError)
	opt := bindOptions(flagSet)
	flagSet.Parse(os.Args[1:])
	if opt.verbose {
		flag.CommandLine.Set("alsologtostderr", "true")
		flag.CommandLine.Set("v", "10")
	}
	if opt.help {
		fmt.Printf(usage)
		flagSet.SetOutput(os.Stdout)
		flagSet.Usage()
		os.Exit(0)
	}

	if err := opt.Validate(); err != nil {
		fmt.Printf("error: %v\n", err)
		opt.writeFailingJUnit(err)
		os.Exit(1)
	}

	if err := opt.Complete(); err != nil {
		fmt.Printf("error: %v\n", err)
		opt.writeFailingJUnit(err)
		os.Exit(1)
	}

	if err := opt.Run(); err != nil {
		fmt.Printf("error: %v\n", err)
		opt.writeFailingJUnit(err)
		os.Exit(1)
	}
}

type stringSlice struct {
	values []string
}

func (s *stringSlice) String() string {
	return strings.Join(s.values, string(filepath.Separator))
}

func (s *stringSlice) Set(value string) error {
	s.values = append(s.values, value)
	return nil
}

type options struct {
	configSpecPath    string
	templatePaths     stringSlice
	secretDirectories stringSlice

	targets stringSlice
	promote bool

	verbose bool
	help    bool
	dry     bool
	print   bool

	writeParams string
	artifactDir string

	gitRef              string
	namespace           string
	baseNamespace       string
	extraInputHash      stringSlice
	idleCleanupDuration time.Duration
	cleanupDuration     time.Duration

	inputHash     string
	secrets       []*coreapi.Secret
	templates     []*templateapi.Template
	configSpec    *api.ReleaseBuildConfiguration
	jobSpec       *api.JobSpec
	clusterConfig *rest.Config

	givePrAuthorAccessToNamespace bool
	impersonateUser               string
	authors                       []string
}

func bindOptions(flag *flag.FlagSet) *options {
	opt := &options{
		idleCleanupDuration: time.Duration(1 * time.Hour),
		cleanupDuration:     time.Duration(12 * time.Hour),
	}

	// command specific options
	flag.BoolVar(&opt.help, "h", false, "short for --help")
	flag.BoolVar(&opt.help, "help", false, "See help for this command.")
	flag.BoolVar(&opt.verbose, "v", false, "Show verbose output.")

	// what we will run
	flag.StringVar(&opt.configSpecPath, "config", "", "The configuration file. If not specified the CONFIG_SPEC environment variable will be used.")
	flag.Var(&opt.targets, "target", "One or more targets in the configuration to build. Only steps that are required for this target will be run.")
	flag.BoolVar(&opt.dry, "dry-run", opt.dry, "Print the steps that would be run and the objects that would be created without executing any steps")
	flag.BoolVar(&opt.print, "print-graph", opt.print, "Print a directed graph of the build steps and exit. Intended for use with the golang digraph utility.")

	// add to the graph of things we run or create
	flag.Var(&opt.templatePaths, "template", "A set of paths to optional templates to add as stages to this job. Each template is expected to contain at least one restart=Never pod. Parameters are filled from environment or from the automatic parameters generated by the operator.")
	flag.Var(&opt.secretDirectories, "secret-dir", "One or more directories that should converted into secrets in the test namespace. If the directory contains a single file with name .dockercfg or config.json it becomes a pull secret.")

	// the target namespace and cleanup behavior
	flag.Var(&opt.extraInputHash, "input-hash", "Add arbitrary inputs to the build input hash to make the created namespace unique.")
	flag.StringVar(&opt.namespace, "namespace", "", "Namespace to create builds into, defaults to build_id from JOB_SPEC. If the string '{id}' is in this value it will be replaced with the build input hash.")
	flag.StringVar(&opt.baseNamespace, "base-namespace", "stable", "Namespace to read builds from, defaults to stable.")
	flag.DurationVar(&opt.idleCleanupDuration, "delete-when-idle", opt.idleCleanupDuration, "If no pod is running for longer than this interval, delete the namespace. Set to zero to retain the contents. Requires the namespace TTL controller to be deployed.")
	flag.DurationVar(&opt.cleanupDuration, "delete-after", opt.cleanupDuration, "If namespace exists for longer than this interval, delete the namespace. Set to zero to retain the contents. Requires the namespace TTL controller to be deployed.")

	// actions to add to the graph
	flag.BoolVar(&opt.promote, "promote", false, "When all other targets complete, publish the set of images built by this job into the release configuration.")

	// output control
	flag.StringVar(&opt.artifactDir, "artifact-dir", "", "If set grab artifacts from test and template jobs.")
	flag.StringVar(&opt.writeParams, "write-params", "", "If set write an env-compatible file with the output of the job.")

	// experimental flags
	flag.StringVar(&opt.gitRef, "git-ref", "", "Populate the job spec from this local Git reference. If JOB_SPEC is set, the refs field will be overwritten.")
	flag.BoolVar(&opt.givePrAuthorAccessToNamespace, "give-pr-author-access-to-namespace", false, "Give view access to the temporarily created namespace to the PR author.")
	flag.StringVar(&opt.impersonateUser, "as", "", "Username to impersonate")

	return opt
}

func (o *options) Validate() error {
	return nil
}

func (o *options) Complete() error {
	config, err := load.Config(o.configSpecPath)
	if err != nil {
		return fmt.Errorf("failed to load configuration: %v", err)
	}
	o.configSpec = config

	if err := o.configSpec.Validate(); err != nil {
		return err
	}

	jobSpec, err := api.ResolveSpecFromEnv()
	if err == nil && jobSpec.Refs != nil {
		for _, pull := range jobSpec.Refs.Pulls {
			o.authors = append(o.authors, pull.Author)
		}
	}

	if err != nil {
		if len(o.gitRef) == 0 {
			return fmt.Errorf("failed to determine job spec: no --git-ref passed and failed to resolve job spec from env: %v", err)
		}
		// Failed to read $JOB_SPEC but --git-ref was passed, so try that instead
		spec, refErr := jobSpecFromGitRef(o.gitRef)
		if refErr != nil {
			return fmt.Errorf("failed to determine job spec: failed to resolve --git-ref: %v", refErr)
		}
		jobSpec = spec
	} else if len(o.gitRef) > 0 {
		// Read from $JOB_SPEC but --git-ref was also passed, so merge them
		spec, err := jobSpecFromGitRef(o.gitRef)
		if err != nil {
			return fmt.Errorf("failed to determine job spec: failed to resolve --git-ref: %v", err)
		}
		jobSpec.Refs = spec.Refs
	}
	jobSpec.BaseNamespace = o.baseNamespace
	o.jobSpec = jobSpec

	if o.dry && o.verbose {
		config, _ := yaml.Marshal(o.configSpec)
		log.Printf("Resolved configuration:\n%s", string(config))
		job, _ := json.Marshal(o.jobSpec)
		log.Printf("Resolved job spec:\n%s", string(job))
	}

	var refs []api.Refs
	if o.jobSpec.Refs != nil {
		refs = append(refs, *o.jobSpec.Refs)
	}
	refs = append(refs, o.jobSpec.ExtraRefs...)

	if len(refs) == 0 {
		log.Printf("No source defined")
	}
	for _, ref := range refs {
		log.Printf(summarizeRef(ref))
	}

	for _, path := range o.secretDirectories.values {
		secret := &coreapi.Secret{Data: make(map[string][]byte)}
		secret.Type = coreapi.SecretTypeOpaque
		secret.Name = filepath.Base(path)
		files, err := ioutil.ReadDir(path)
		if err != nil {
			return fmt.Errorf("could not read dir %s for secret: %v", path, err)
		}
		for _, f := range files {
			if f.IsDir() {
				continue
			}
			path := filepath.Join(path, f.Name())
			// if the file is a broken symlink or a symlink to a dir, skip it
			if fi, err := os.Stat(path); err != nil || fi.IsDir() {
				continue
			}
			secret.Data[f.Name()], err = ioutil.ReadFile(path)
			if err != nil {
				return fmt.Errorf("could not read file %s for secret: %v", path, err)
			}
		}
		if len(secret.Data) == 1 {
			if _, ok := secret.Data[coreapi.DockerConfigJsonKey]; ok {
				secret.Type = coreapi.SecretTypeDockerConfigJson
			}
			if _, ok := secret.Data[coreapi.DockerConfigKey]; ok {
				secret.Type = coreapi.SecretTypeDockercfg
			}
		}
		o.secrets = append(o.secrets, secret)
	}

	for _, path := range o.templatePaths.values {
		contents, err := ioutil.ReadFile(path)
		if err != nil {
			return fmt.Errorf("could not read dir %s for template: %v", path, err)
		}
		obj, gvk, err := templatescheme.Codecs.UniversalDeserializer().Decode(contents, nil, nil)
		if err != nil {
			return fmt.Errorf("unable to parse template %s: %v", path, err)
		}
		template, ok := obj.(*templateapi.Template)
		if !ok {
			return fmt.Errorf("%s is not a template: %v", path, gvk)
		}
		if len(template.Name) == 0 {
			template.Name = filepath.Base(path)
			template.Name = strings.TrimSuffix(template.Name, filepath.Ext(template.Name))
		}
		o.templates = append(o.templates, template)
	}

	clusterConfig, err := loadClusterConfig()
	if err != nil {
		return fmt.Errorf("failed to load cluster config: %v", err)
	}

	if len(o.impersonateUser) > 0 {
		clusterConfig.Impersonate = rest.ImpersonationConfig{UserName: o.impersonateUser}
	}

	o.clusterConfig = clusterConfig

	return nil
}

func (o *options) Run() error {
	start := time.Now()
	defer func() {
		log.Printf("Ran for %s", time.Now().Sub(start).Truncate(time.Second))
	}()

	// load the graph from the configuration
	buildSteps, postSteps, err := defaults.FromConfig(o.configSpec, o.jobSpec, o.templates, o.writeParams, o.artifactDir, o.promote, o.clusterConfig, o.targets.values)
	if err != nil {
		return fmt.Errorf("failed to generate steps from config: %v", err)
	}

	ctx, cancel := context.WithCancel(context.Background())

	handler := func(s os.Signal) {
		if o.dry {
			os.Exit(0)
		}
		log.Printf("error: Process interrupted with signal %s, exiting in 10s ...", s)
		cancel()
		time.Sleep(10 * time.Second)
		os.Exit(1)
	}

	return interrupt.New(handler, o.saveNamespaceArtifacts).Run(func() error {
		// Before we create the namespace, we need to ensure all inputs to the graph
		// have been resolved. We must run this step before we resolve the partial
		// graph or otherwise two jobs with different targets would create different
		// artifact caches.
		if err := o.resolveInputs(ctx, buildSteps); err != nil {
			return fmt.Errorf("could not resolve inputs: %v", err)
		}

		if err := o.writeMetadataJSON(); err != nil {
			return fmt.Errorf("unable to write metadata.json for build: %v", err)
		}

		if o.print {
			if err := api.PrintDigraph(os.Stdout, buildSteps); err != nil {
				return fmt.Errorf("could not print graph: %v", err)
			}
			return nil
		}

		// convert the full graph into the subset we must run
		nodes, err := api.BuildPartialGraph(buildSteps, o.targets.values)
		if err != nil {
			return fmt.Errorf("could not build execution graph: %v", err)
		}

		if err := printExecutionOrder(nodes); err != nil {
			return fmt.Errorf("could not print execution order: %v", err)
		}

		// initialize the namespace if necessary and create any resources that must
		// exist prior to execution
		if err := o.initializeNamespace(); err != nil {
			return fmt.Errorf("could not initialize namespace: %v", err)
		}

		client, err := coreclientset.NewForConfig(o.clusterConfig)
		if err != nil {
			return fmt.Errorf("could not get core client for cluster config: %v", err)
		}
		eventRecorder := eventRecorder(client, o.namespace)
		runtimeObject := &coreapi.ObjectReference{Namespace: o.namespace}

		if !o.dry {
			eventRecorder.Event(runtimeObject, coreapi.EventTypeNormal, "CiJobStarted", eventJobDescription(o.jobSpec, o.namespace))
		}
		// execute the graph
		suites, err := steps.Run(ctx, nodes, o.dry)
		if err := o.writeJUnit(suites, "operator"); err != nil {
			log.Printf("warning: Unable to write JUnit result: %v", err)
		}
		if err != nil {
			if !o.dry {
				eventRecorder.Event(runtimeObject, coreapi.EventTypeWarning, "CiJobFailed", eventJobDescription(o.jobSpec, o.namespace))
				time.Sleep(time.Second)
			}
			return errWroteJUnit{fmt.Errorf("could not run steps: %v", err)}
		}

		for _, step := range postSteps {
			if err := step.Run(ctx, o.dry); err != nil {
				if !o.dry {
					eventRecorder.Event(runtimeObject, coreapi.EventTypeWarning, "PostStepFailed",
						fmt.Sprintf("Post step %s failed while %s", step.Name(), eventJobDescription(o.jobSpec, o.namespace)))
					time.Sleep(time.Second)
				}
				return fmt.Errorf("could not run post step %s: %v", step.Name(), err)
			}
		}

		if !o.dry {
			eventRecorder.Event(runtimeObject, coreapi.EventTypeNormal, "CiJobSucceeded", eventJobDescription(o.jobSpec, o.namespace))
			time.Sleep(time.Second)
		}

		return nil
	})
}

// loadClusterConfig loads connection configuration
// for the cluster we're deploying to. We prefer to
// use in-cluster configuration if possible, but will
// fall back to using default rules otherwise.
func loadClusterConfig() (*rest.Config, error) {
	clusterConfig, err := rest.InClusterConfig()
	if err == nil {
		return clusterConfig, nil
	}

	credentials, err := clientcmd.NewDefaultClientConfigLoadingRules().Load()
	if err != nil {
		return nil, fmt.Errorf("could not load credentials from config: %v", err)
	}

	clusterConfig, err = clientcmd.NewDefaultClientConfig(*credentials, &clientcmd.ConfigOverrides{}).ClientConfig()
	if err != nil {
		return nil, fmt.Errorf("could not load client configuration: %v", err)
	}
	return clusterConfig, nil
}

func (o *options) resolveInputs(ctx context.Context, steps []api.Step) error {
	var inputs api.InputDefinition
	for _, step := range steps {
		definition, err := step.Inputs(ctx, o.dry)
		if err != nil {
			return fmt.Errorf("could not determine inputs for step %s: %v", step.Name(), err)
		}
		inputs = append(inputs, definition...)
	}

	// a change in the config for the build changes the output
	configSpec, err := yaml.Marshal(o.configSpec)
	if err != nil {
		panic(err)
	}
	inputs = append(inputs, string(configSpec))
	if len(o.extraInputHash.values) > 0 {
		inputs = append(inputs, o.extraInputHash.values...)
	}

	// add the binary modification time and size (in lieu of a content hash)
	path, _ := exec.LookPath(os.Args[0])
	if len(path) == 0 {
		path = os.Args[0]
	}
	if stat, err := os.Stat(path); err == nil {
		glog.V(4).Infof("Using binary as hash: %s %d %d", path, stat.ModTime().UTC().Unix(), stat.Size())
		inputs = append(inputs, fmt.Sprintf("%d-%d", stat.ModTime().UTC().Unix(), stat.Size()))
	} else {
		glog.V(4).Infof("Could not calculate info from current binary to add to input hash: %v", err)
	}

	sort.Strings(inputs)
	o.inputHash = inputHash(inputs)

	// input hash is unique for a given job definition and input refs
	if len(o.namespace) == 0 {
		o.namespace = "ci-op-{id}"
	}
	o.namespace = strings.Replace(o.namespace, "{id}", o.inputHash, -1)
	// TODO: instead of mutating this here, we should pass the parts of graph execution that are resolved
	// after the graph is created but before it is run down into the run step.
	o.jobSpec.Namespace = o.namespace
	log.Printf("Using namespace %s", o.namespace)

	return nil
}

func (o *options) initializeNamespace() error {
	if o.dry {
		return nil
	}
	projectGetter, err := projectclientset.NewForConfig(o.clusterConfig)
	if err != nil {
		return fmt.Errorf("could not get project client for cluster config: %v", err)
	}

	log.Printf("Creating namespace %s", o.namespace)
	retries := 5
	for {
		project, err := projectGetter.ProjectV1().ProjectRequests().Create(&projectapi.ProjectRequest{
			ObjectMeta: meta.ObjectMeta{
				Name: o.namespace,
			},
			DisplayName: fmt.Sprintf("%s - %s", o.namespace, o.jobSpec.Job),
			Description: jobDescription(o.jobSpec, o.configSpec),
		})
		if err != nil && !kerrors.IsAlreadyExists(err) {
			return fmt.Errorf("could not set up namespace for test: %v", err)
		}
		if err != nil {
			project, err = projectGetter.ProjectV1().Projects().Get(o.namespace, meta.GetOptions{})
			if err != nil {
				if kerrors.IsNotFound(err) {
					continue
				}
				// wait a few seconds for auth caches to catch up
				if kerrors.IsForbidden(err) && retries > 0 {
					retries--
					time.Sleep(time.Second)
					continue
				}
				return fmt.Errorf("cannot retrieve test namespace: %v", err)
			}
		}
		if project.Status.Phase == coreapi.NamespaceTerminating {
			log.Println("Waiting for namespace to finish terminating before creating another")
			time.Sleep(3 * time.Second)
			continue
		}
		break
	}

	if o.givePrAuthorAccessToNamespace {
		// Generate rolebinding for all the PR Authors.
		rbacClient, err := rbacclientset.NewForConfig(o.clusterConfig)
		if err != nil {
			return fmt.Errorf("could not get RBAC client for cluster config: %v", err)
		}
		for _, author := range o.authors {
			log.Printf("Creating rolebinding for user %s in namespace %s", author, o.namespace)
			if _, err := rbacClient.RoleBindings(o.namespace).Create(&rbacapi.RoleBinding{
				ObjectMeta: meta.ObjectMeta{
					Name:      "ci-op-author-access",
					Namespace: o.namespace,
				},
				Subjects: []rbacapi.Subject{{Kind: "User", Name: author}},
				RoleRef: rbacapi.RoleRef{
					Kind: "ClusterRole",
					Name: "admin",
				},
			}); err != nil && !kerrors.IsAlreadyExists(err) {
				return fmt.Errorf("could not create role binding for: %v", err)
			}
		}
	}

	client, err := coreclientset.NewForConfig(o.clusterConfig)
	if err != nil {
		return fmt.Errorf("could not get core client for cluster config: %v", err)
	}

	updates := map[string]string{}
	if o.idleCleanupDuration > 0 {
		log.Printf("Setting a soft TTL of %s for the namespace\n", o.idleCleanupDuration.String())
		updates["ci.openshift.io/ttl.soft"] = o.idleCleanupDuration.String()
	}

	if o.cleanupDuration > 0 {
		log.Printf("Setting a hard TTL of %s for the namespace\n", o.cleanupDuration.String())
		updates["ci.openshift.io/ttl.hard"] = o.cleanupDuration.String()
	}

	// This label makes sure that the namespace is active, and the value will be updated
	// if the namespace will be reused.
	updates["ci.openshift.io/active"] = time.Now().Format(time.RFC3339)

	if len(updates) > 0 {
		if err := retry.RetryOnConflict(retry.DefaultRetry, func() error {
			ns, err := client.Namespaces().Get(o.namespace, meta.GetOptions{})
			if err != nil {
				return err
			}

			if ns.Annotations == nil {
				ns.Annotations = make(map[string]string)
			}
			for key, value := range updates {
				ns.ObjectMeta.Annotations[key] = value
			}

			_, updateErr := client.Namespaces().Update(ns)
			if kerrors.IsForbidden(updateErr) {
				log.Printf("warning: Could not add annotations because you do not have permission to update the namespace (details: %v)", updateErr)
				return nil
			}
			return updateErr
		}); err != nil {
			return fmt.Errorf("could not update namespace to add TTLs and active annotations: %v", err)
		}
	}

	log.Printf("Setting up pipeline imagestream for the test")
	imageGetter, err := imageclientset.NewForConfig(o.clusterConfig)
	if err != nil {
		return fmt.Errorf("could not get image client for cluster config: %v", err)
	}

	// create the image stream or read it to get its uid
	is, err := imageGetter.ImageStreams(o.jobSpec.Namespace).Create(&imageapi.ImageStream{
		ObjectMeta: meta.ObjectMeta{
			Namespace: o.jobSpec.Namespace,
			Name:      api.PipelineImageStream,
		},
		Spec: imageapi.ImageStreamSpec{
			// pipeline:* will now be directly referenceable
			LookupPolicy: imageapi.ImageLookupPolicy{Local: true},
		},
	})
	if err != nil {
		if !kerrors.IsAlreadyExists(err) {
			return fmt.Errorf("could not set up pipeline imagestream for test: %v", err)
		}
		is, _ = imageGetter.ImageStreams(o.jobSpec.Namespace).Get(api.PipelineImageStream, meta.GetOptions{})
	}
	if is != nil {
		isTrue := true
		o.jobSpec.SetOwner(&meta.OwnerReference{
			APIVersion: "image.openshift.io/v1",
			Kind:       "ImageStream",
			Name:       api.PipelineImageStream,
			UID:        is.UID,
			Controller: &isTrue,
		})
	}

	for _, secret := range o.secrets {
		_, err := client.Secrets(o.namespace).Create(secret)
		if kerrors.IsAlreadyExists(err) {
			existing, err := client.Secrets(o.namespace).Get(secret.Name, meta.GetOptions{})
			if err != nil {
				return fmt.Errorf("could not retrieve secret %s: %v", secret.Name, err)
			}
			for k, v := range secret.Data {
				existing.Data[k] = v
			}
			if _, err := client.Secrets(o.namespace).Update(existing); err != nil {
				return fmt.Errorf("could not update secret %s: %v", secret.Name, err)
			}
			log.Printf("Updated secret %s", secret.Name)
			continue
		}
		if err != nil {
			return fmt.Errorf("could not create secret %s: %v", secret.Name, err)
		}
		log.Printf("Created secret %s", secret.Name)
	}
	return nil
}

// prowResultMetadata is the set of metadata consumed by testgrid and
// gubernator after a CI run completes. We add work-namespace as our
// target namespace for the job.
//
// Example from k8s:
//
// "metadata": {
// 	"repo-commit": "253f03e0055b6649f8b25e84122748d39a284141",
// 	"node_os_image": "cos-stable-65-10323-64-0",
// 	"repos": {
// 		"k8s.io/kubernetes": "master:1c04caa04325e1f64d9a15714ad61acdd2a81013,71936:353a0b391d6cb0c26e1c0c6b180b300f64039e0e",
// 		"k8s.io/release": "master"
// 	},
// 	"infra-commit": "de7741746",
// 	"repo": "k8s.io/kubernetes",
// 	"master_os_image": "cos-stable-65-10323-64-0",
// 	"job-version": "v1.14.0-alpha.0.1012+253f03e0055b66",
// 	"pod": "dd8d320f-ff64-11e8-b091-0a580a6c02ef"
// }
//
type prowResultMetadata struct {
	RepoCommit    string            `json:"repo-commit"`
	Repo          string            `json:"repo"`
	Repos         map[string]string `json:"repos"`
	InfraCommit   string            `json:"infra-commit"`
	JobVersion    string            `json:"job-version"`
	Pod           string            `json:"pod"`
	WorkNamespace string            `json:"work-namespace"`
}

func (o *options) writeMetadataJSON() error {
	if len(o.artifactDir) == 0 {
		return nil
	}

	m := prowResultMetadata{}

	if o.jobSpec.Refs != nil {
		m.Repo = fmt.Sprintf("%s/%s", o.jobSpec.Refs.Org, o.jobSpec.Refs.Repo)
		m.Repos = map[string]string{m.Repo: o.jobSpec.Refs.String()}
	}
	if len(o.jobSpec.ExtraRefs) > 0 {
		if m.Repos == nil {
			m.Repos = make(map[string]string)
		}
		for _, ref := range o.jobSpec.ExtraRefs {
			repo := fmt.Sprintf("%s/%s", ref.Org, ref.Repo)
			if _, ok := m.Repos[repo]; ok {
				continue
			}
			m.Repos[repo] = ref.String()
		}
	}

	m.Pod = o.jobSpec.ProwJobID
	m.WorkNamespace = o.namespace

	data, err := json.MarshalIndent(m, "", "  ")
	if err != nil {
		return err
	}
	if o.dry {
		log.Printf("metadata.json:\n%s", string(data))
		return nil
	}
	return ioutil.WriteFile(filepath.Join(o.artifactDir, "metadata.json"), data, 0640)
}

// errWroteJUnit indicates that this error is covered by existing JUnit output and writing
// another JUnit file is not necessary (in writeFailingJUnit)
type errWroteJUnit struct {
	error
}

// writeFailingJUnit attempts to write a JUnit artifact when the graph could not be
// initialized in order to capture the result for higher level automation.
func (o *options) writeFailingJUnit(err error) {
	if _, ok := err.(errWroteJUnit); ok {
		return
	}
	suites := &junit.TestSuites{
		Suites: []*junit.TestSuite{
			{
				NumTests:  1,
				NumFailed: 1,
				TestCases: []*junit.TestCase{
					{
						Name: "initialize",
						FailureOutput: &junit.FailureOutput{
							Output: err.Error(),
						},
					},
				},
			},
		},
	}
	if err := o.writeJUnit(suites, "job"); err != nil {
		glog.V(4).Infof("Unable to write top level failing JUnit artifact")
	}
}

func (o *options) writeJUnit(suites *junit.TestSuites, name string) error {
	if len(o.artifactDir) == 0 || suites == nil {
		return nil
	}
	suites.Suites[0].Name = name
	out, err := xml.MarshalIndent(suites, "", "  ")
	if err != nil {
		return fmt.Errorf("could not marshal jUnit XML: %v", err)
	}
	return ioutil.WriteFile(filepath.Join(o.artifactDir, fmt.Sprintf("junit_%s.xml", name)), out, 0640)
}

// oneWayEncoding can be used to encode hex to a 62-character set (0 and 1 are duplicates) for use in
// short display names that are safe for use in kubernetes as resource names.
var oneWayNameEncoding = base32.NewEncoding("bcdfghijklmnpqrstvwxyz0123456789").WithPadding(base32.NoPadding)

// inputHash returns a string that hashes the unique parts of the input to avoid collisions.
func inputHash(inputs api.InputDefinition) string {
	hash := sha256.New()

	// the inputs form a part of the hash
	for _, s := range inputs {
		hash.Write([]byte(s))
	}

	// Object names can't be too long so we truncate
	// the hash. This increases chances of collision
	// but we can tolerate it as our input space is
	// tiny.
	return oneWayNameEncoding.EncodeToString(hash.Sum(nil)[:5])
}

// saveNamespaceArtifacts is a best effort attempt to save ci-operator namespace artifacts to disk
// for review later.
func (o *options) saveNamespaceArtifacts() {
	if len(o.artifactDir) == 0 {
		return
	}

	namespaceDir := filepath.Join(o.artifactDir, "build-resources")
	if err := os.Mkdir(namespaceDir, 0777); err != nil {
		log.Printf("Unable to create build-resources directory: %v", err)
		return
	}

	if kubeClient, err := coreclientset.NewForConfig(o.clusterConfig); err == nil {
		pods, _ := kubeClient.Pods(o.namespace).List(meta.ListOptions{})
		data, _ := json.MarshalIndent(pods, "", "  ")
		ioutil.WriteFile(filepath.Join(namespaceDir, "pods.json"), data, 0644)
		events, _ := kubeClient.Events(o.namespace).List(meta.ListOptions{})
		data, _ = json.MarshalIndent(events, "", "  ")
		ioutil.WriteFile(filepath.Join(namespaceDir, "events.json"), data, 0644)
	}

	if buildClient, err := buildclientset.NewForConfig(o.clusterConfig); err == nil {
		builds, _ := buildClient.Builds(o.namespace).List(meta.ListOptions{})
		data, _ := json.MarshalIndent(builds, "", "  ")
		ioutil.WriteFile(filepath.Join(namespaceDir, "builds.json"), data, 0644)
	}

	if imageClient, err := imageclientset.NewForConfig(o.clusterConfig); err == nil {
		if err != nil {
			return
		}
		imagestreams, _ := imageClient.ImageStreams(o.namespace).List(meta.ListOptions{})
		data, _ := json.MarshalIndent(imagestreams, "", "  ")
		ioutil.WriteFile(filepath.Join(namespaceDir, "imagestreams.json"), data, 0644)
	}

	if templateClient, err := templateclientset.NewForConfig(o.clusterConfig); err == nil {
		templateInstances, _ := templateClient.TemplateInstances(o.namespace).List(meta.ListOptions{})
		data, _ := json.MarshalIndent(templateInstances, "", "  ")
		ioutil.WriteFile(filepath.Join(namespaceDir, "templateinstances.json"), data, 0644)
	}
}

// eventJobDescription returns a string representing the pull requests and authors description, to be used in events.
func eventJobDescription(jobSpec *api.JobSpec, namespace string) string {
	pulls := []string{}
	authors := []string{}

	if jobSpec.Refs == nil {
		return fmt.Sprintf("Running job %s in namespace %s", jobSpec.Job, namespace)
	}
	if len(jobSpec.Refs.Pulls) == 1 {
		pull := jobSpec.Refs.Pulls[0]
		return fmt.Sprintf("Running job %s for PR https://github.com/%s/%s/pull/%d in namespace %s from author %s",
			jobSpec.Job, jobSpec.Refs.Org, jobSpec.Refs.Repo, pull.Number, namespace, pull.Author)
	}
	for _, pull := range jobSpec.Refs.Pulls {
		pulls = append(pulls, fmt.Sprintf("https://github.com/%s/%s/pull/%d", jobSpec.Refs.Org, jobSpec.Refs.Repo, pull.Number))
		authors = append(authors, pull.Author)
	}
	return fmt.Sprintf("Running job %s for PRs (%s) in namespace %s from authors (%s)",
		jobSpec.Job, strings.Join(pulls, ", "), namespace, strings.Join(authors, ", "))
}

// jobDescription returns a string representing the job's description.
func jobDescription(job *api.JobSpec, config *api.ReleaseBuildConfiguration) string {
	if job.Refs == nil {
		return fmt.Sprintf("%s", job.Job)
	}
	var links []string
	for _, pull := range job.Refs.Pulls {
		links = append(links, fmt.Sprintf("https://github.com/%s/%s/pull/%d - %s", job.Refs.Org, job.Refs.Repo, pull.Number, pull.Author))
	}
	if len(links) > 0 {
		return fmt.Sprintf("%s\n\n%s on https://github.com/%s/%s", strings.Join(links, "\n"), job.Job, job.Refs.Org, job.Refs.Repo)
	}
	return fmt.Sprintf("%s on https://github.com/%s/%s ref=%s commit=%s", job.Job, job.Refs.Org, job.Refs.Repo, job.Refs.BaseRef, job.Refs.BaseSHA)
}

func jobSpecFromGitRef(ref string) (*api.JobSpec, error) {
	parts := strings.Split(ref, "@")
	if len(parts) != 2 {
		return nil, fmt.Errorf("must be ORG/NAME@REF")
	}
	prefix := strings.Split(parts[0], "/")
	if len(prefix) != 2 {
		return nil, fmt.Errorf("must be ORG/NAME@REF")
	}
	repo := fmt.Sprintf("https://github.com/%s/%s.git", prefix[0], prefix[1])
	out, err := exec.Command("git", "ls-remote", repo, parts[1]).Output()
	if err != nil {
		return nil, fmt.Errorf("'git ls-remote %s %s' failed with '%s'", repo, parts[1], err)
	}
	resolved := strings.Split(strings.Split(string(out), "\n")[0], "\t")
	sha := resolved[0]
	if len(sha) == 0 {
		return nil, fmt.Errorf("ref '%s' does not point to any commit in '%s'", parts[1], parts[0])
	}
	// sanity check that regular refs are fully determined
	if strings.HasPrefix(resolved[1], "refs/heads/") && !strings.HasPrefix(parts[1], "refs/heads/") {
		if resolved[1] != ("refs/heads/" + parts[1]) {
			trimmed := resolved[1][len("refs/heads/"):]
			// we could fix this for the user, but better to require them to be explicit
			return nil, fmt.Errorf("ref '%s' does not point to any commit in '%s' (did you mean '%s'?)", parts[1], parts[0], trimmed)
		}
	}
	log.Printf("Resolved %s to commit %s", ref, sha)
	return &api.JobSpec{Type: api.PeriodicJob, Job: "dev", Refs: &api.Refs{Org: prefix[0], Repo: prefix[1], BaseRef: parts[1], BaseSHA: sha}}, nil
}

func printExecutionOrder(nodes []*api.StepNode) error {
	ordered, err := api.TopologicalSort(nodes)
	if err != nil {
		return fmt.Errorf("could not sort nodes: %v", err)
	}
	log.Printf("Running %s", strings.Join(api.NodeNames(ordered), ", "))
	return nil
}

var shaRegex = regexp.MustCompile(`^[0-9a-fA-F]+$`)

// shorten takes a string, and if it looks like a hexadecimal Git SHA it truncates it to
// l characters. The values provided to job spec are not required to be SHAs but could also be
// tags or other git refs.
func shorten(value string, l int) string {
	if len(value) > l && shaRegex.MatchString(value) {
		return value[:l]
	}
	return value
}

func summarizeRef(refs api.Refs) string {
	if len(refs.Pulls) > 0 {
		var pulls []string
		for _, pull := range refs.Pulls {
			pulls = append(pulls, fmt.Sprintf("#%d %s @%s", pull.Number, shorten(pull.SHA, 8), pull.Author))
		}
		return fmt.Sprintf("Resolved source https://github.com/%s/%s to %s@%s, merging: %s", refs.Org, refs.Repo, refs.BaseRef, shorten(refs.BaseSHA, 8), strings.Join(pulls, ", "))
	}
	return fmt.Sprintf("Resolved source https://github.com/%s/%s to %s@%s", refs.Org, refs.Repo, refs.BaseRef, shorten(refs.BaseSHA, 8))
}

func eventRecorder(kubeClient *coreclientset.CoreV1Client, namespace string) record.EventRecorder {
	eventBroadcaster := record.NewBroadcaster()
	eventBroadcaster.StartRecordingToSink(&coreclientset.EventSinkImpl{
		Interface: coreclientset.New(kubeClient.RESTClient()).Events("")})
	return eventBroadcaster.NewRecorder(
		templatescheme.Scheme, coreapi.EventSource{Component: namespace})
}
